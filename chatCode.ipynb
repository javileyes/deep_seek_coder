{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usando Transformers con cuantificación de 8 bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para utilizar la cuantificación de 8 bits en modelos de Transformers, puedes usar el argumento `load_in_8bit=True` en el método `from_pretrained`. Esto te permite cargar un modelo reduciendo aproximadamente a la mitad los requisitos de memoria. Este método es compatible con modelos que admiten la carga con la biblioteca Accelerate y que contienen capas `torch.nn.Linear`.\n",
    "\n",
    "NOTA: Necesita tener instalado accelerete bitsandbytes para funcionar\n",
    "```bash\n",
    "pip install accelerete bitsandbytes\n",
    "```\n",
    "\n",
    "Para modificar tu script y aplicar cuantificación de 8 bits, puedes hacer lo siguiente:\n",
    "\n",
    "Este código utiliza cuantificación de 8 bits en lugar de la configuración original de 4 bits, lo que debería resultar en una reducción sustancial en el uso de memoria mientras mantiene un equilibrio adecuado entre eficiencia y precisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar modelo y ejecutar inferencia (mejor cargarlo una vez y luego ejecutarlo varias veces -en la siguiente sección)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c6cc66899740a3b939820e821ff94e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here is a simple implementation of the Quick Sort algorithm in Python:\n",
      "\n",
      "```python\n",
      "def quick_sort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    else:\n",
      "        pivot = arr[0]\n",
      "        less_than_pivot = [x for x in arr[1:] if x <= pivot]\n",
      "        greater_than_pivot = [x for x in arr[1:] if x > pivot]\n",
      "        return quick_sort(less_than_pivot) + [pivot] + quick_sort(greater_than_pivot)\n",
      "\n",
      "# Test the function\n",
      "print(quick_sort([3, 6, 8, 10, 1, 2, 1]))\n",
      "```\n",
      "\n",
      "This code works by selecting a 'pivot' element from the array and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot. The pivot element is then in its final position. The process is then recursively applied to the sub-arrays.\n",
      "<|EOT|>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", device_map='auto', load_in_8bit=True, trust_remote_code=True)\n",
    "\n",
    "#gracias al parámetro device_map='auto' en la función from_pretrained. Este parámetro le indica a la biblioteca Transformers que asigne automáticamente el modelo al mejor dispositivo disponible (normalmente la GPU, si hay una). Por lo tanto, no se necesita llamar explícitamente a .to('cuda')\n",
    "\n",
    "messages = [\n",
    "    {'role': 'user', 'content': \"write a quick sort algorithm in python.\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=32021)\n",
    "print(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Inicializa la variable model como None al inicio\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "\n",
    "# Función para cargar el modelo si aún no está cargado\n",
    "def load_model():\n",
    "    global model\n",
    "    global tokenizer\n",
    "    if model is None or not hasattr(model, 'num_parameters'):  # Verifica si model está vacío o no parece ser un modelo válido\n",
    "        print(\"Cargando modelo...\")       \n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", device_map='auto', load_in_8bit=True, trust_remote_code=True)\n",
    "        print(\"Modelo cargado.\")\n",
    "    else:\n",
    "        print(\"Modelo ya estaba cargado.\")\n",
    "\n",
    "load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencia Modelo con gestión de chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo ya estaba cargado.\n",
      "El problema de las torres de Hanoi es un problema de recursividad. Consiste en mover un n�mero n de discos de un poste a otro, con la restricción de que un disco más grande no puede estar por encima de un disco más pequeño.\n",
      "\n",
      "Aquí está el algoritmo para resolver el problema de las torres de Hanoi:\n",
      "\n",
      "1. Mover n-1 discos de la torre origen a la torre auxiliar, usando la torre destino.\n",
      "2. Mover el disco restante de la torre origen a la torre destino.\n",
      "3. Mover los n-1 discos de la torre auxiliar a la torre destino usando la torre origen.\n",
      "\n",
      "Aquí está el pseudocódigo para el problema de las torres de Hanoi:\n",
      "\n",
      "```\n",
      "procedure hanoi(n, source, target, auxiliary)\n",
      "   if n > 0 then begin\n",
      "      // Mover los n-1 discos de la torre source a la torre auxiliary, usando target como torre auxiliar\n",
      "      hanoi(n - 1, source, auxiliary, target);\n",
      "      \n",
      "      // Mover el disco restante de la torre source a la torre target\n",
      "      move disk from source to target;\n",
      "      \n",
      "      // Mover los n-1 discos de la torre auxiliary a la torre target usando source como torre source\n",
      "      hanoi(n - 1, auxiliary, target, source);\n",
      "   end\n",
      "endprocedure\n",
      "```\n",
      "\n",
      "Ejemplo de ejecución:\n",
      "\n",
      "Si queremos mover 3 discos de la torre A a la torre C usando la torre B como torre auxiliar, el algoritmo se ejecutaría de la siguiente manera:\n",
      "\n",
      "1. Mover 2 discos de la torre A a la torre B usando la torre C como torre auxiliar.\n",
      "2. Mover el disco restante de la torre A a la torre C.\n",
      "3. Mover los 2 discos de la torre B a la torre C usando la torre A como torre source.\n",
      "4. Mover el disco restante de la torre A a la torre C.\n",
      "5. Mover 1 disco de la torre B a la torre C.\n",
      "\n",
      "Por lo tanto, para resolver el problema de las torres de Hanoi con 3 discos, se moverían 7 movimientos.\n",
      "\n",
      "Este es un ejemplo sencillo y para discos de tamaño diferente, el n�mero de movimientos seguirá siendo 2^n - 1, donde n es el n�mero de discos.\n",
      "\n",
      "\n",
      "################################################\n",
      "\n",
      "Sí, puedo hacerlo en Python. Aquí está la implementación del algoritmo de las torres de Hanoi en Python:\n",
      "\n",
      "```python\n",
      "def hanoi(n, source, target, auxiliary):\n",
      "    if n > 0:\n",
      "        # Mover los n-1 discos de la torre source a la torre auxiliary, usando target como torre auxiliar\n",
      "        hanoi(n - 1, source, auxiliary, target)\n",
      "        \n",
      "        # Mover el disco restante de la torre source a la torre target\n",
      "        print(f\"Mover disco {n} de la torre {source} a la torre {target}\")\n",
      "        \n",
      "        # Mover los n-1 discos de la torre auxiliary a la torre target usando source como torre source\n",
      "        hanoi(n - 1, auxiliary, target, source)\n",
      "\n",
      "# Ejecutar el algoritmo con 3 discos\n",
      "hanoi(3, 'A', 'C', 'B')\n",
      "```\n",
      "\n",
      "En este código, `hanoi` es una función recursiva que resuelve el problema de las torres de Hanoi. La función recibe cuatro parámetros: el n�mero de discos `n`, la torre origen `source`, la torre destino `target` y la torre auxiliar `auxiliary`.\n",
      "\n",
      "La función imprime los pasos para mover cada disco, de la forma \"Mover disco `n` de la torre `source` a la torre `target`\".\n",
      "\n",
      "Por ejemplo, si ejecutamos `hanoi(3, 'A', 'C', 'B')`, el programa imprimirá los siguientes pasos:\n",
      "\n",
      "```\n",
      "Mover disco 1 de la torre A a la torre B\n",
      "Mover disco 2 de la torre A a la torre C\n",
      "Mover disco 1 de la torre B a la torre C\n",
      "Mover disco 3 de la torre A a la torre C\n",
      "Mover disco 1 de la torre C a la torre B\n",
      "Mover disco 2 de la torre C a la torre A\n",
      "Mover disco 1 de la torre B a la torre A\n",
      "```\n",
      "\n",
      "Estos son los pasos para mover 3 discos de la torre A a la torre C usando la torre B como torre auxiliar.\n",
      "\n",
      "\n",
      "################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "\n",
    "def ajustar_contexto(texto, max_longitud=2000, secuencia=\"### Instruction\"):\n",
    "    # Comprobar si la longitud del texto es mayor que el máximo permitido\n",
    "    if len(texto) > max_longitud:\n",
    "        # Buscar la secuencia de ajuste\n",
    "        indice_secuencia = texto.find(secuencia)\n",
    "\n",
    "        # Si la secuencia existe\n",
    "        if indice_secuencia != -1:\n",
    "            # Retornar el texto desde la secuencia en adelante\n",
    "            return texto[indice_secuencia:]\n",
    "        else:\n",
    "            # Si la secuencia no se encuentra, recortar simplemente a la longitud máxima\n",
    "            return texto[:max_longitud]\n",
    "    else:\n",
    "        return texto\n",
    "\n",
    "# Ejemplo de uso de la función\n",
    "# texto_engordado = \"Este es el texto adicional que podría hacer que el texto sobrepase los 30 caracteres. ### Instruction Continuación del texto...\"\n",
    "# texto_ajustado = ajustar_contexto(texto_engordado, 30)\n",
    "# print(texto_ajustado)\n",
    "\n",
    "def generate_long_chat(historico, input_text, max_additional_tokens=2000, stop=[\"<|EOT|>\"]):\n",
    "\n",
    "\n",
    "    prompt = f\"### Instruction:\\n{input_text}\\n### Response:\\n\"\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True) # para streamear el output pero sin repetir el prompt ni el contexto anterior. \n",
    "\n",
    "    final_prompt = historico + \"\\n\" + prompt\n",
    "    longitud_prompt_tokens = len(tokenizer.encode(final_prompt))\n",
    "\n",
    "    inputs = tokenizer(final_prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "\n",
    "    model_inputs = inputs.to(model.device)      # .to(\"cuda\")\n",
    "    outputs = model.generate(**model_inputs,\n",
    "                             streamer=streamer,\n",
    "                             max_new_tokens=max_additional_tokens,\n",
    "                            #  max_length=max_length,\n",
    "                             temperature=0.1,\n",
    "                             pad_token_id = 3200,\n",
    "                             eos_token_id=32021,\n",
    "                             do_sample=True)\n",
    "\n",
    "    # Decodificar el tensor de salida a una cadena de texto\n",
    "    inicio_generado = longitud_prompt_tokens - 1\n",
    "    decoded_output = tokenizer.decode(outputs[0][inicio_generado:], skip_special_tokens=True)  \n",
    "    # decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)    \n",
    "    \n",
    "    text = final_prompt + decoded_output + \"<|EOT|>\"\n",
    "    return text\n",
    "\n",
    "\n",
    "# para que pueda tener suficiente memoria si queremos algo más de contexto.\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", device_map='auto', load_in_8bit=True, trust_remote_code=True)\n",
    "\n",
    "# modelo sin cuantizar (se queda sin memoria con contexto grande)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", torch_dtype=\"auto\", trust_remote_code=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", device_map='auto', torch_dtype=\"auto\", trust_remote_code=True)\n",
    "#torch_dtype=torch.float16  (half precision) or torch.float32 (single precision que sería absurdo porque deepseek coder viene de llama 2 cuyo parámetros son float16)\n",
    "\n",
    "load_model()\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are an expert AI programming assistant, utilizing the DeepSeek Coder model, and you only answer questions related to computer science.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "historico = system_prompt\n",
    "\n",
    "while True:\n",
    "    # read input\n",
    "    input_text = input(\"user: \")\n",
    "    if input_text == \"/exit\": break\n",
    "    if input_text == \"/historico\": \n",
    "        print(historico)\n",
    "        continue\n",
    "    if input_text == \"/len\": \n",
    "        print(\"longitud del contexto en caracteres: \", len(historico))\n",
    "        continue\n",
    "    if input_text == \"/clear\":\n",
    "        historico = \"\"\n",
    "        continue\n",
    "    # generate response\n",
    "    historico = generate_long_chat(historico, input_text=input_text, max_additional_tokens=2048)\n",
    "    historico = ajustar_contexto(historico)\n",
    "    # print response\n",
    "    # print(salida)\n",
    "    print(f\"\\n################################################\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usando Transformers con cuantificación de 4 bits (BitsandBytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La librería Bits and Bytes (BitsAndBytes) es una herramienta que facilita la optimización de modelos de lenguaje grandes (como GPT-3 y similares) para su ejecución en hardware con recursos limitados. Estas optimizaciones incluyen técnicas como la cuantificación a 4 bits y el uso de tipos de datos de menor precisión para reducir el uso de memoria y mejorar la eficiencia de la inferencia. \n",
    "\n",
    "Veamos en detalle los parámetros que mencionaste en tu configuración de `BitsAndBytesConfig`:\n",
    "\n",
    "1. **`load_in_4bit`**: \n",
    "   - Este parámetro, cuando se establece en `True`, habilita la cuantificación a 4 bits. La cuantificación es una técnica para reducir la precisión de los números usados en un modelo (por ejemplo, de 32 bits a 4 bits). Esto disminuye el tamaño del modelo y puede reducir el uso de memoria, pero potencialmente a costa de alguna pérdida de precisión en los cálculos.\n",
    "\n",
    "2. **`bnb_4bit_quant_type`**:\n",
    "   - Define el tipo de cuantificación a 4 bits a utilizar. El valor `'nf4'` se refiere a una forma específica de cuantificación a 4 bits desarrollada por el equipo de BitsAndBytes. Esta forma de cuantificación está diseñada para mantener un equilibrio entre el rendimiento y la precisión.\n",
    "\n",
    "3. **`bnb_4bit_use_double_quant`**:\n",
    "   - Cuando se establece en `True`, activa el uso de \"doble cuantificación\" en el proceso de cuantificación a 4 bits. Esto significa que se aplican dos rondas de cuantificación en lugar de una, lo que puede mejorar la precisión de la representación de 4 bits a costa de una ligera sobrecarga computacional.\n",
    "\n",
    "4. **`bnb_4bit_compute_dtype`**:\n",
    "   - Este parámetro especifica el tipo de dato a utilizar para los cálculos internos del modelo cuando se utiliza la cuantificación a 4 bits. El tipo de dato `bfloat16` es una representación de punto flotante de 16 bits con menos precisión que el estándar de 32 bits (`float32`), pero con un rango comparable. Usar `bfloat16` puede ofrecer un buen equilibrio entre el rendimiento y la precisión, y es especialmente útil en hardware que soporta operaciones de `bfloat16` de manera eficiente.\n",
    "\n",
    "En resumen, estos parámetros están configurando tu modelo para que use cuantificación a 4 bits con un método específico y una doble cuantificación para mejorar la precisión, todo mientras realiza cálculos internos en un tipo de dato de menor precisión (`bfloat16`) para mejorar la eficiencia. Estas optimizaciones están diseñadas para permitir que modelos grandes se ejecuten en hardware con memoria limitada, como GPUs con menos VRAM, manteniendo un buen equilibrio entre el rendimiento y la precisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d656cf3bf984a6bbd2f040c9f4d966a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here is a simple implementation of the Quick Sort algorithm in Python:\n",
      "\n",
      "```python\n",
      "def quick_sort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    else:\n",
      "        pivot = arr[len(arr) // 2]\n",
      "        left = [x for x in arr if x < pivot]\n",
      "        middle = [x for x in arr if x == pivot]\n",
      "        right = [x for x in arr if x > pivot]\n",
      "        return quick_sort(left) + middle + quick_sort(right)\n",
      "\n",
      "# Test the function\n",
      "print(quick_sort([3,6,8,10,1,2,1]))\n",
      "```\n",
      "\n",
      "This function works by selecting a pivot element from the array and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot. The sub-arrays are then recursively sorted.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from torch import bfloat16\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", quantization_config=bnb_config,     device_map='auto', trust_remote_code=True)\n",
    "messages=[\n",
    "    { 'role': 'user', 'content': \"write a quick sort algorithm in python.\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
    "# 32021 is the id of <|EOT|> token\n",
    "outputs = model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=32021)\n",
    "print(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usando Transformers con cuantificación de 16 bits (convirtiendo a half-precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al usar parámetro torch_dtype=\"auto\"  en la función from_pretrained() de la biblioteca Transformers de Hugging Face es una funcionalidad relativamente reciente que permite a la función determinar automáticamente el tipo de datos de PyTorch (dtype) más adecuado para cargar el modelo, basándose en el hardware disponible.\n",
    "\n",
    "Cuando configuras torch_dtype en \"auto\", la biblioteca intentará elegir el mejor tipo de datos para optimizar el rendimiento y la eficiencia de memoria. Por ejemplo, si se detecta una GPU compatible con half-precision (como las GPUs con soporte para Tensor Cores de NVIDIA), puede cargar automáticamente el modelo en \"float16\" para aprovechar la aceleración de half-precision. Si no se detecta dicho hardware, utilizará el tipo de datos estándar (usualmente float32).\n",
    "\n",
    "Automáticamente se carga usando half-precision (16 bits) cuando se usa la biblioteca Accelerate. Esto se debe a que Accelerate utiliza la biblioteca NVIDIA Apex para la conversión automática a half-precision. Puedes ver esto en la documentación de Accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: accelerate\n",
      "Version: 0.24.1\n",
      "Summary: Accelerate\n",
      "Home-page: https://github.com/huggingface/accelerate\n",
      "Author: The HuggingFace team\n",
      "Author-email: sylvain@huggingface.co\n",
      "License: Apache\n",
      "Location: /home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages\n",
      "Requires: huggingface-hub, numpy, packaging, psutil, pyyaml, torch\n",
      "Required-by: peft\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la biblioteca Transformers de Hugging Face, los valores para `device_map` y `torch_dtype` se pueden definir de la siguiente manera:\n",
    "\n",
    "1. **`device_map`**:\n",
    "   - **`'auto'`**: El modelo se distribuye automáticamente a través de los dispositivos disponibles de manera óptima. Esto es útil si tienes múltiples GPUs y deseas que la biblioteca maneje la distribución del modelo de manera eficiente.\n",
    "   - **Diccionario específico de dispositivos**: Puedes proporcionar un diccionario que asigne capas específicas del modelo a dispositivos específicos. Por ejemplo, `{0: [0, 1], 1: [2, 3]}` asignaría las capas 0 y 1 a la GPU 0 y las capas 2 y 3 a la GPU 1.\n",
    "   - **Lista de IDs de dispositivos**: También puedes proporcionar una lista de IDs de dispositivos para un reparto más manual.\n",
    "\n",
    "2. **`torch_dtype`**:\n",
    "   - **`'auto'`**: Selecciona automáticamente el tipo de datos de PyTorch más adecuado en función del hardware disponible y el modelo específico.\n",
    "   - **Tipos de datos específicos de PyTorch**: Puedes especificar un tipo de datos concreto como `torch.float32`, `torch.float16`, etc., para controlar la precisión y el uso de memoria del modelo.\n",
    "\n",
    "Estos parámetros proporcionan flexibilidad en la configuración del hardware y el rendimiento del modelo, permitiendo optimizaciones según el entorno de ejecución y los requisitos específicos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí en un modelo de 32 capas típico Llama asignamos la mitad de las capas a la GPU y la otra mitad a la CPU:\n",
    "```python\n",
    "device_map = {\n",
    "    \"cuda:0\": [0, 1, 2, ..., 15],  # Primeras 16 capas en la GPU\n",
    "    \"cpu\": [16, 17, 18, ..., 31]   # Últimas 16 capas en la CPU\n",
    "}\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\"tu-modelo\", device_map=device_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## usamos device_map='auto' para que la biblioteca maneje la distribución del modelo de manera eficiente.\n",
    "Esta manera es muy sencilla, intenta poner el modelo en el mejor dispositivo PERO si no cupiera todo el modelo en la GPU daría un error. Con este parámetro a \"auto\" se asigna un disposivivo que se puede verse con \"model.device\" \n",
    "\n",
    "Otra forma de gestionar sería sin usar el parámetro device_map, entonces todo se cargaría en la RAM y a partir de ahí se podría usar instrucciones como:\n",
    "```python\n",
    "model = model.to(dtype=torch.float16, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "```\n",
    "para asignar el tipo de dato y el dispositivo donde volcar el modelo.\n",
    "Esta forma postergaría tanto la cuantificación a half_precision como la carga a la GPU hasta esta linea.\n",
    "\n",
    "Si no tenemos memoria suficiente en la RAM se podría usar \"dtype=torch.float16\" O \"dtype='auto'\" durante la carga del modelo y luego usar\n",
    "```python\n",
    "model = model.to(device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "```\n",
    "para trasladar el modelo a la GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mínima cuantificación para mayor precisión: \"torch_dtype=torch.float16\" o \"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ff72edebcb4a91aacbf048f6e083e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", torch_dtype=\"auto\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", device_map='auto', torch_dtype=\"auto\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here is a simple implementation of the Quick Sort algorithm in Python:\n",
      "\n",
      "```python\n",
      "def quick_sort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    else:\n",
      "        pivot = arr[0]\n",
      "        less_than_pivot = [x for x in arr[1:] if x <= pivot]\n",
      "        greater_than_pivot = [x for x in arr[1:] if x > pivot]\n",
      "        return quick_sort(less_than_pivot) + [pivot] + quick_sort(greater_than_pivot)\n",
      "\n",
      "# Test the function\n",
      "arr = [10, 7, 8, 9, 1, 5]\n",
      "print(\"Original array:\", arr)\n",
      "print(\"Sorted array:\", quick_sort(arr))\n",
      "```\n",
      "\n",
      "This code works by selecting a 'pivot' element from the array and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot. The pivot element is then in its final position. The process is then repeated for the sub-arrays.\n",
      "<|EOT|>\n"
     ]
    }
   ],
   "source": [
    "messages=[\n",
    "    { 'role': 'user', 'content': \"write a quick sort algorithm in python.\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
    "# 32021 is the id of <|EOT|> token\n",
    "outputs = model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=32021)\n",
    "print(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para ver el formato que usa: https://github.com/deepseek-ai/deepseek-coder\n",
    "\n",
    "```bash\n",
    "You are an AI programming assistant, utilizing the DeepSeek Coder model, developed by DeepSeek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer.\n",
    "### Instruction:\n",
    "['content']\n",
    "### Response:\n",
    "['content']\n",
    "<|EOT|>\n",
    "### Instruction:\n",
    "['content']\n",
    "### Response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer\n",
      "### Instruction:\n",
      "write a quick sort algorithm in python.\n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'user', 'content': \"write a quick sort algorithm in python.\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Decodificar el tensor completo a texto\n",
    "text = tokenizer.decode(inputs.flatten(), skip_special_tokens=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ver capas del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo tiene 32 capas.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "def count_model_layers(model):\n",
    "    #En nuestro caso entrará en este primer if\n",
    "    if hasattr(model, 'model') and hasattr(model.model, 'layers'):\n",
    "        return len(model.model.layers)\n",
    "    elif hasattr(model, 'encoder'):\n",
    "        return len(model.encoder.layer)\n",
    "    elif hasattr(model, 'transformer'):\n",
    "        return len(model.transformer.h)\n",
    "    elif hasattr(model, 'decoder'):\n",
    "        return len(model.decoder.block)\n",
    "    else:\n",
    "        return \"No se puede determinar el número de capas\"\n",
    "\n",
    "\n",
    "print(f\"El modelo tiene {count_model_layers(model)} capas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usamos device_map \"a medida\" para intentar que el modelo \"quepa\" en la GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No he logrado que funcionen para este modelo, dan error, solo funcionan con \"auto\" (investigar más)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97cc50a2dff3444d980f6464ba9da318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device_map = {\n",
    "    \"cuda:0\": [0, 1, 2, ..., 15],  # Primeras 16 capas en la GPU\n",
    "    \"cpu\": [16, 17, 18, ..., 31]   # Últimas 16 capas en la CPU\n",
    "}\n",
    "device_map = {\n",
    "    \"cuda:0\": [0, 1, 2, ..., 15],  # Primeras 16 capas en la GPU\n",
    "    \"cpu\": \"default\"   # resto de capas en la CPU\n",
    "}\n",
    "\n",
    "device_map = {\"cuda:0\": \"default\"}\n",
    "\n",
    "#SOLO CONSIGO QUE FUNCIONE \"DEVICE_MAP\" CON AUTO (TENGO QUE INVESTIGAR MÁS)\n",
    "\n",
    "device_map = \"auto\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", torch_dtype=\"auto\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", device_map=device_map, torch_dtype=\"auto\", trust_remote_code=True)\n",
    "\n",
    "\n",
    "# model = LlamaForCausalLM.from_pretrained(\"tu-modelo\", device_map=device_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otra forma de hacerlo es convertirlo a half-precision después de cargarlo pero necesitas tener más memoria RAM disponible.\n",
    "Teniendo la mitad de VRAM (GPU) que de RAM se podría cargar un modelo grande y después convertirlo a half-precision (16 bits) para poder ejecutarlo en la GPU.\n",
    "\n",
    "Para modificar tu script para usar cuantificación de 16 bits (también conocida como precisión mixta o half-precision), puedes convertir el modelo a float16 después de cargarlo. Esto se puede hacer utilizando el método .to() disponible en modelos de PyTorch. Aquí está el código ajustado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True)\n",
    "\n",
    "# Convertir el modelo a float16 y transferirlo a la GPU si está disponible\n",
    "model = model.to(dtype=torch.float16, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "messages = [\n",
    "    {'role': 'user', 'content': \"write a quick sort algorithm in python.\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=32021)\n",
    "print(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencia de DeepSeek Coder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a Python implementation of the quick sort algorithm:\n",
      "\n",
      "```python\n",
      "def quickSort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    pivot = arr[len(arr) // 2]\n",
      "    left = [x for x in arr if x < pivot]\n",
      "    middle = [x for x in arr if x == pivot]\n",
      "    right = [x for x in arr if x > pivot]\n",
      "    return quickSort(left) + middle + quickSort(right)\n",
      "\n",
      "print(quickSort([3,6,8,10,1,2,1]))\n",
      "# Output: [1, 1, 2, 3, 6, 8, 10]\n",
      "```\n",
      "\n",
      "The quick sort algorithm works by selecting a 'pivot' element from the array and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot. The sub-arrays are then recursively sorted.\n",
      "\n",
      "This implementation is a bit more complex than necessary for a quick sort, because it creates additional lists. A more space-efficient version of quick sort might involve swapping elements in-place, although this can be more complex to implement.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'user', 'content': \"write a quick sort algorithm in python.\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(inputs, max_new_tokens=512, do_sample=True, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=32021)\n",
    "print(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## uso de streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a basic implementation of Quick Sort algorithm in Python:\n",
      "\n",
      "```python\n",
      "def quicksort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    else:\n",
      "        pivot = arr[0]\n",
      "        less_than_pivot = [x for x in arr[1:] if x <= pivot]\n",
      "        greater_than_pivot = [x for x in arr[1:] if x > pivot]\n",
      "        return quicksort(less_than_pivot) + [pivot] + quicksort(greater_than_pivot)\n",
      "\n",
      "# Test the function\n",
      "arr = [3, 6, 8, 10, 1, 2, 1]\n",
      "print(\"Original Array: \", arr)\n",
      "print(\"Sorted Array: \", quicksort(arr))\n",
      "```\n",
      "\n",
      "In this implementation, the `quicksort` function takes in a list `arr` and:\n",
      "\n",
      "- If the list is empty or contains only one element, it is already sorted, so it just returns the list.\n",
      "- It selects the first element of the list as the pivot and creates two lists: `less_than_pivot` and `greater_than_pivot`.\n",
      "- It then recursively sorts the `less_than_pivot` and `greater_than_pivot` lists, and combines the three lists to form the final sorted array.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True) # para streamear el output pero sin repetir el prompt ni el contexto anterior.\n",
    "\n",
    "content = \"\"\"\n",
    "write a quick sort algorithm in python.\n",
    "\"\"\"\n",
    "messages = [\n",
    "    {'role': 'user', 'content': content}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(inputs, max_new_tokens=512, streamer=streamer, do_sample=True, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=32021)\n",
    "# print(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varias secuencias de salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secuencia 1:\n",
      "Here is a quick sort algorithm implemented in Python:\n",
      "\n",
      "```python\n",
      "def quickSort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    else:\n",
      "        pivot = arr[len(arr) // 2]\n",
      "        left = [x for x in arr if x < pivot]\n",
      "        middle = [x for x in arr if x == pivot]\n",
      "        right = [x for x in arr if x > pivot]\n",
      "        return quickSort(left) + middle + quickSort(right)\n",
      "\n",
      "# Testing the function\n",
      "print(quickSort([3,6,8,10,1,2,1]))\n",
      "# Output: [1, 1, 2, 3, 6, 8, 10]\n",
      "```\n",
      "\n",
      "The quickSort function works by selecting a 'pivot' element from the array and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot. The sub-arrays are then recursively sorted.\n",
      "\n",
      "\n",
      "Secuencia 2:\n",
      "Sure, here is a Python implementation of the Quick Sort algorithm:\n",
      "\n",
      "```python\n",
      "def quicksort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    pivot = arr[len(arr) // 2]\n",
      "    left = [x for x in arr if x < pivot]\n",
      "    middle = [x for x in arr if x == pivot]\n",
      "    right = [x for x in arr if x > pivot]\n",
      "    return quicksort(left) + middle + quicksort(right)\n",
      "\n",
      "print(quicksort([3,6,8,10,1,2,1]))\n",
      "```\n",
      "\n",
      "This algorithm works by selecting a 'pivot' element from the array and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot. The sub-arrays are then recursively sorted.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "content = \"\"\"\n",
    "write a quick sort algorithm in python.\n",
    "\"\"\"\n",
    "messages = [\n",
    "    {'role': 'user', 'content': content}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(inputs, max_new_tokens=512, do_sample=True, top_k=50, top_p=0.95, num_return_sequences=2, eos_token_id=32021)\n",
    "\n",
    "print(\"Secuencia 1:\")\n",
    "print(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))\n",
    "\n",
    "print(\"\\nSecuencia 2:\")\n",
    "print(tokenizer.decode(outputs[1][len(inputs[0]):], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Busqueda de la mejor secuencia de salida (Beam Search) [tarda bastante más pero puede ser más precisa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secuencia 1:\n",
      "Sure, here is a simple implementation of the Quick Sort algorithm in Python:\n",
      "\n",
      "```python\n",
      "def quick_sort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    else:\n",
      "        pivot = arr[0]\n",
      "        less_than_pivot = [x for x in arr[1:] if x <= pivot]\n",
      "        greater_than_pivot = [x for x in arr[1:] if x > pivot]\n",
      "        return quick_sort(less_than_pivot) + [pivot] + quick_sort(greater_than_pivot)\n",
      "\n",
      "# Test the function\n",
      "print(quick_sort([3, 6, 8, 10, 1, 2, 1]))  # Output: [1, 1, 2, 3, 6, 8, 10]\n",
      "```\n",
      "\n",
      "This algorithm works by selecting a 'pivot' element from the array and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot. The sub-arrays are then recursively sorted.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "content = \"\"\"\n",
    "write a quick sort algorithm in python.\n",
    "\"\"\"\n",
    "messages = [\n",
    "    {'role': 'user', 'content': content}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(inputs, max_new_tokens=512, do_sample=True, top_k=50, top_p=0.95, num_beams=4, num_return_sequences=1, eos_token_id=32021)\n",
    "\n",
    "print(\"Secuencia 1:\")\n",
    "print(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usando llama-cpp-python (sin uso de Docker) (NO FUNCIONA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# echo `pwd`\n",
    "cd ./llama.cpp/examples/\n",
    "./chat.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4080, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ./models/deepseek-coder-6.7b-instruct.Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q6_K     [  4096, 32256,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:              blk.0.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:              blk.0.attn_k.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:         blk.0.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:            blk.0.ffn_gate.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:              blk.0.ffn_up.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:              blk.1.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:              blk.1.attn_k.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:         blk.1.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.ffn_gate.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:              blk.1.ffn_up.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:              blk.2.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:              blk.2.attn_k.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:         blk.2.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:            blk.2.ffn_gate.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:              blk.2.ffn_up.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:              blk.3.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:              blk.3.attn_k.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:         blk.3.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:            blk.3.ffn_gate.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:              blk.3.ffn_up.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:              blk.4.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:              blk.4.attn_k.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:         blk.4.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:            blk.4.ffn_gate.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:              blk.4.ffn_up.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:              blk.5.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:              blk.5.attn_k.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:         blk.5.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:            blk.5.ffn_gate.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:              blk.5.ffn_up.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:              blk.6.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:              blk.6.attn_k.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:         blk.6.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:            blk.6.ffn_gate.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:              blk.6.ffn_up.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:              blk.7.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:              blk.7.attn_k.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:         blk.7.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:            blk.7.ffn_gate.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:              blk.7.ffn_up.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:              blk.8.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:              blk.8.attn_k.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:         blk.8.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:            blk.8.ffn_gate.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:              blk.8.ffn_up.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:              blk.9.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:              blk.9.attn_k.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:         blk.9.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:            blk.9.ffn_gate.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:              blk.9.ffn_up.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:             blk.10.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:             blk.10.attn_k.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:        blk.10.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:           blk.10.ffn_gate.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:             blk.10.ffn_up.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:             blk.11.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:             blk.11.attn_k.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:        blk.11.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:           blk.11.ffn_gate.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:             blk.11.ffn_up.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:             blk.12.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:             blk.12.attn_k.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:        blk.12.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:           blk.12.ffn_gate.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:             blk.12.ffn_up.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:             blk.13.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:             blk.13.attn_k.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:        blk.13.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:           blk.13.ffn_gate.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:             blk.13.ffn_up.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:             blk.14.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:             blk.14.attn_k.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:        blk.14.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:           blk.14.ffn_gate.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:             blk.14.ffn_up.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:             blk.15.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:             blk.15.attn_k.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:        blk.15.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.15.ffn_gate.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:             blk.15.ffn_up.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:             blk.16.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:             blk.16.attn_k.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:        blk.16.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.16.ffn_gate.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:             blk.16.ffn_up.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:             blk.17.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:             blk.17.attn_k.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:        blk.17.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:           blk.17.ffn_gate.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:             blk.17.ffn_up.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:             blk.18.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:             blk.18.attn_k.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:        blk.18.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:           blk.18.ffn_gate.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:             blk.18.ffn_up.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:             blk.19.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:             blk.19.attn_k.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:        blk.19.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:           blk.19.ffn_gate.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:             blk.19.ffn_up.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:             blk.20.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:             blk.20.attn_k.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:        blk.20.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:           blk.20.ffn_gate.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:             blk.20.ffn_up.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:             blk.21.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:             blk.21.attn_k.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:        blk.21.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:           blk.21.ffn_gate.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:             blk.21.ffn_up.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:             blk.22.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:             blk.22.attn_k.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:        blk.22.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:           blk.22.ffn_gate.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:             blk.22.ffn_up.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:             blk.23.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:             blk.23.attn_k.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:        blk.23.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:           blk.23.ffn_gate.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:             blk.23.ffn_up.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:             blk.24.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:             blk.24.attn_k.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:        blk.24.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:           blk.24.ffn_gate.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:             blk.24.ffn_up.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:             blk.25.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:             blk.25.attn_k.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:        blk.25.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:           blk.25.ffn_gate.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:             blk.25.ffn_up.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.26.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:             blk.26.attn_k.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:        blk.26.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:           blk.26.ffn_gate.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:             blk.26.ffn_up.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:             blk.27.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:             blk.27.attn_k.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:        blk.27.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:           blk.27.ffn_gate.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:             blk.27.ffn_up.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:             blk.28.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:             blk.28.attn_k.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:        blk.28.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:           blk.28.ffn_gate.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:             blk.28.ffn_up.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:             blk.29.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:             blk.29.attn_k.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:        blk.29.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:           blk.29.ffn_gate.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:             blk.29.ffn_up.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:             blk.30.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:             blk.30.attn_k.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:        blk.30.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:           blk.30.ffn_gate.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:             blk.30.ffn_up.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:             blk.31.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:             blk.31.attn_k.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:        blk.31.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:           blk.31.ffn_gate.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:             blk.31.ffn_up.weight q6_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:                    output.weight q6_K     [  4096, 32256,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = deepseek-ai_deepseek-coder-6.7b-instruct\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 16384\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 100000.000000\n",
      "llama_model_loader: - kv  11:                    llama.rope.scale_linear f32              = 4.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32256]   = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32256]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32256]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,31757]   = [\"Ġ Ġ\", \"Ġ t\", \"Ġ a\", \"i n\", \"h e...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 32013\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 32021\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 32014\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q6_K:  226 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 243/32256 vs 237/32256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 32256\n",
      "llm_load_print_meta: n_merges         = 31757\n",
      "llm_load_print_meta: n_ctx_train      = 16384\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 100000.0\n",
      "llm_load_print_meta: freq_scale_train = 0.25\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 16384\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly Q6_K\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 5.15 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name   = deepseek-ai_deepseek-coder-6.7b-instruct\n",
      "llm_load_print_meta: BOS token = 32013 '<｜begin▁of▁sentence｜>'\n",
      "llm_load_print_meta: EOS token = 32021 '<|EOT|>'\n",
      "llm_load_print_meta: PAD token = 32014 '<｜end▁of▁sentence｜>'\n",
      "llm_load_print_meta: LF token  = 126 'Ä'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  = 5274.09 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/35 layers to GPU\n",
      "llm_load_tensors: VRAM used: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "llm = Llama(model_path=\"./models/deepseek-coder-6.7b-instruct.Q6_K.gguf\", device_map=\"auto\")\n",
    "output = llm(\n",
    "    \"Q: Name the planets in the solar system? A: \", # Prompt\n",
    "    max_tokens=32, # Generate up to 32 tokens\n",
    "    stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "    echo=True # Echo the prompt back in the output\n",
    ") # Generate a completion, can also call create_completion\n",
    "print(output)\n",
    "# {\n",
    "#   \"id\": \"cmpl-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n",
    "#   \"object\": \"text_completion\",\n",
    "#   \"created\": 1679561337,\n",
    "#   \"model\": \"./models/7B/llama-model.gguf\",\n",
    "#   \"choices\": [\n",
    "#     {\n",
    "#       \"text\": \"Q: Name the planets in the solar system? A: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune and Pluto.\",\n",
    "#       \"index\": 0,\n",
    "#       \"logprobs\": None,\n",
    "#       \"finish_reason\": \"stop\"\n",
    "#     }\n",
    "#   ],\n",
    "#   \"usage\": {\n",
    "#     \"prompt_tokens\": 14,\n",
    "#     \"completion_tokens\": 28,\n",
    "#     \"total_tokens\": 42\n",
    "#   }\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-82d11f81-51ab-4402-97ce-cc85c3a4ebb5', 'object': 'text_completion', 'created': 1701680303, 'model': './models/deepseek-coder-6.7b-instruct.Q6_K.gguf', 'choices': [{'text': 'Q: Name the planets in the solar system? A: \\nTerra, Saturno, Urano e Netuno.\\nB: Mercrio, Venus, Marte, Jpiter, Saturno, Urano e Netuno.\\nC: Terra, Vênus, Marte, Jpiter, Saturno, Urano, Netuno e Plutão.\\nD: Mercurio, Vénus, Mars, Jupiter, Saturn, Uranus and Neptune.\\nE: Mercrio, Venus, Terra, Marte, Jpiter, Saturno, Urano e Netuno.\\nF: Não é uma alternativa válida pois o planeta Plutão não está na lista. \\nG: Está errada pois a letra G não faz parte dos planetas do sistema solar. \\nAcertou: Falso, mas não sabemos qual foi a resposta correta.\\n\\nQuem pode ajudar?\\nOBS: Esta pergunta pertence ao grupo de discusses no Quora (https://www.quora.com/q/topic/Planet-Names) e está sendo migranda para o StackExchange em 2016.\\n\\nA: Terra, Vênus, Marte, Jpiter, Saturno, Urano e Netuno.\\n\\nEsses são os nomes de todos os planetas no sistema solar, incluindo Plutão (que nem sempre é considerado um planeta). \\nA letra G não faz parte dos planetas do sistema solar, pois ela não pertence a nenhum alfabeto. Por isso, seria errada afirmar que \"G\" é uma alternativa válida para responder à pergunta.\\n\\nA: Falso, mas não sabemos qual foi a resposta correta.\\n\\nEssa afirmação parece estar desatualizada. A maioria das fontes confianças mais atuais e consolidadas já adotou o padrão da letra-nmero para designaçes de planetas', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 14, 'completion_tokens': 498, 'total_tokens': 512}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     820.72 ms\n",
      "llama_print_timings:      sample time =      68.68 ms /   498 runs   (    0.14 ms per token,  7250.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   43896.59 ms /   498 runs   (   88.15 ms per token,    11.34 tokens per second)\n",
      "llama_print_timings:       total time =   44593.99 ms\n"
     ]
    }
   ],
   "source": [
    "output = llm(\n",
    "    \"Q: Name the planets in the solar system? A: \", # Prompt\n",
    "    max_tokens=512, # Generate up to 32 tokens\n",
    "    # stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "    echo=True # Echo the prompt back in the output\n",
    ") # Generate a completion, can also call create_completion\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usando llama-cpp-python (No consigo usar la GPU, uso Docker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "docker run  -it -p 8000:8000 --name deepseek -v /mnt/d/DeveloperIA/DeepSeekCoder/models:/models -e MODEL=/models/deepseek-coder-6.7b-instruct.Q6_K.gguf ghcr.io/abetlen/llama-cpp-python:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## entrar dentro del docker con bash\n",
    "docker exec -it deepseek bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EJecutar inferencia mediante api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#include<iostream>\n",
      "using namespace std;\n",
      "int partition(int A[], int start, int end) {\n",
      "    int pivot = A[end]; // taking last element as the pivot \n",
      "    int pIndex= start; // pointer for greater elements\n",
      "    for (int i = start; i < end; i++){ // traverse through all array elements \n",
      "        if(A[i] <= pivot) {// if current item is smaller than the pivot, swap it with the greater element pointed by pIndex\n",
      "            swap(A[i], A[pIndex]); \n",
      "            ++pIndex;\n",
      "        }\n",
      "    }\n",
      "    // swapping pivot to its final position\n",
      "    swap (A[end], A[pIndex]);\n",
      "    return pIndex;\n",
      "}\n",
      "void QuickSort(int A[], int start, int end) {\n",
      "    if(start < end){ \n",
      "        int pIndex = partition(A, start, end); // call the partition function to get the pivot index  \n",
      "        QuickSort(A, start , pIndex -1 ); // recursively call for elements on left of pivot\n",
      "        QuickSort(A, pIndex + 1, end);  // recursively call for elements on right side of pivot\n",
      "    }\n",
      "}\n",
      "int main(){\n",
      "    int A[] = {7,2,1,6,8,5,3,4};// initialize an array\n",
      "    QuickSort (A,0,7); // Calling the quicksort function\n",
      "    for(int i=0;i<8;i++) \n",
      "        cout<<A[i]<<\" \"; // print the sorted elements of array\n",
      "    return 0 ;\n",
      "}\n",
      "```\n",
      "The output will be:  1 2 3 4 5 6 7 8, which is the ascending order after sorting. The QuickSort algorithm works by selecting a 'pivot' element from the array and partitioning the other elements into two sub-arrays according to whether they are less than or greater than the pivot. Then recursively applying this process to these sub-arrays until the whole array is sorted.\n",
      "\n",
      "This particular implementation of quicksort involves choosing the last item as the pivot, but there are many variations where you could choose other items (such as a random item), and different\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "prompt = \"write a quick sort algorithm in python.\" \n",
    "\n",
    "response = requests.post(\"http://localhost:8000/v1/completions\", json={\n",
    "    \"prompt\": prompt,\n",
    "    \"max_tokens\": 512,\n",
    "    \"eos_token_id\": 32021,\n",
    "    #uso gpu\n",
    "    \"device\": \"cuda:0\"\n",
    "})\n",
    "\n",
    "print(response.json()[\"choices\"][0][\"text\"])\n",
    "# print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing check_cuda.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile check_cuda.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "int main() {\n",
    "    int deviceCount;\n",
    "    cudaGetDeviceCount(&deviceCount);\n",
    "    if (deviceCount > 0) {\n",
    "        printf(\"GPU is available\\n\");\n",
    "    } else {\n",
    "        printf(\"GPU is not available\\n\");\n",
    "    }\n",
    "    return 0;\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc check_cuda.cu -o check_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "!./check_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verificar si tengo funcionando cuBLAS (aceleración de GPU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing check_cublas.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile check_cublas.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cublas_v2.h>\n",
    "\n",
    "int main() {\n",
    "    cublasHandle_t handle;\n",
    "    cublasStatus_t status;\n",
    "\n",
    "    // Inicializar CUBLAS\n",
    "    status = cublasCreate(&handle);\n",
    "    if (status != CUBLAS_STATUS_SUCCESS) {\n",
    "        printf(\"CUBLAS no está instalado o no funciona correctamente.\\n\");\n",
    "        return -1;\n",
    "    }\n",
    "\n",
    "    // Ejemplo sencillo: Escalar un vector\n",
    "    const int n = 10;\n",
    "    float alpha = 2.0f;\n",
    "    float x[n] = {1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0};\n",
    "    float *d_x;\n",
    "\n",
    "    // Asignar memoria en la GPU\n",
    "    cudaMalloc((void **)&d_x, n * sizeof(float));\n",
    "\n",
    "    // Copiar datos al dispositivo\n",
    "    cublasSetVector(n, sizeof(float), x, 1, d_x, 1);\n",
    "\n",
    "    // Escalar el vector x por alpha y almacenar el resultado en x\n",
    "    cublasSscal(handle, n, &alpha, d_x, 1);\n",
    "\n",
    "    // Copiar los resultados de vuelta a la memoria del host\n",
    "    cublasGetVector(n, sizeof(float), d_x, 1, x, 1);\n",
    "\n",
    "    // Limpiar\n",
    "    cudaFree(d_x);\n",
    "    cublasDestroy(handle);\n",
    "\n",
    "    printf(\"CUBLAS está instalado y funcionando correctamente.\\n\");\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este script en C realiza los siguientes pasos:\n",
    "\n",
    "1. Inicializa cuBLAS: Intenta crear un handle de cuBLAS. Si falla, significa que cuBLAS no está disponible.\n",
    "\n",
    "1. Define y Escala un Vector: Define un vector simple y lo escala usando la función cublasSscal.\n",
    "\n",
    "1. Manejo de Memoria: Asigna memoria en la GPU, copia los datos del host al dispositivo, realiza la operación y luego copia los datos de vuelta.\n",
    "\n",
    "1. Limpieza: Libera la memoria y destruye el handle de cuBLAS.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -lcublas -o cublas_test check_cublas.cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUBLAS está instalado y funcionando correctamente.\n"
     ]
    }
   ],
   "source": [
    "!./cublas_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uso de Pytorch o Tensorflow junto con Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La biblioteca transformers de Hugging Face está diseñada para ser compatible tanto con PyTorch como con TensorFlow. Al usar transformers, puedes elegir trabajar con cualquiera de estos frameworks de aprendizaje profundo, dependiendo de tu preferencia y necesidades específicas. Aquí te explico cómo funciona esto:\n",
    "\n",
    "Compatibilidad con PyTorch y TensorFlow:\n",
    "\n",
    "Cuando utilizas transformers, puedes especificar si deseas que los tensores sean compatibles con PyTorch o TensorFlow mediante el parámetro return_tensors. Por ejemplo, return_tensors=\"pt\" te dará tensores compatibles con PyTorch, mientras que return_tensors=\"tf\" te dará tensores compatibles con TensorFlow.\n",
    "Muchos modelos en transformers están disponibles tanto en versiones de PyTorch (AutoModelFor...) como en versiones de TensorFlow (TFAutoModelFor...).\n",
    "Trabajo de Bajo Nivel:\n",
    "\n",
    "Si tu trabajo requiere manipulaciones de bajo nivel de tensores o un control más fino sobre el entrenamiento del modelo, tu elección entre PyTorch y TensorFlow puede depender de tu familiaridad con estos frameworks y sus respectivas características.\n",
    "PyTorch es conocido por su facilidad de uso, especialmente para la investigación y el desarrollo de prototipos, gracias a su naturaleza imperativa y a la forma en que maneja la diferenciación automática.\n",
    "TensorFlow, especialmente en su versión 2 y con la interfaz Keras, también es muy accesible y ofrece un fuerte soporte para la producción y el escalado gracias a su modelo de gráficos computacionales y herramientas como TensorFlow Serving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprobar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(arr, low, high):\n",
    "    i = (low-1)\n",
    "    pivot = arr[high]\n",
    "\n",
    "    for j in range(low, high):\n",
    "        if arr[j] <= pivot:\n",
    "            i = i+1\n",
    "            arr[i], arr[j] = arr[j], arr[i]\n",
    "\n",
    "    arr[i+1], arr[high] = arr[high], arr[i+1]\n",
    "    return (i+1)\n",
    "\n",
    "def quickSort(arr, low, high):\n",
    "    if len(arr) == 1:\n",
    "        return arr\n",
    "    if low < high:\n",
    "        pi = partition(arr, low, high)\n",
    "        quickSort(arr, low, pi-1)\n",
    "        quickSort(arr, pi+1, high)\n",
    "\n",
    "arr = [10, 7, 8, 9, 1, 5]\n",
    "n = len(arr)\n",
    "quickSort(arr, 0, n-1)\n",
    "print(\"Sorted array is:\", arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_sort(arr):\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "    else:\n",
    "        pivot = arr[len(arr) // 2]\n",
    "        left = [x for x in arr if x < pivot]\n",
    "        middle = [x for x in arr if x == pivot]\n",
    "        right = [x for x in arr if x > pivot]\n",
    "        return quick_sort(left) + middle + quick_sort(right)\n",
    "\n",
    "# Example usage:\n",
    "arr = [3, 6, 8, 10, 1, 2, 1]\n",
    "print(quick_sort(arr))  # prints \"[1, 1, 2, 3, 6, 8, 10]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tunel y proxy Socks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importando herramientas\n",
      "../herramientas/\n",
      "/mnt/d/DeveloperIA/DeepSeekCoder\n",
      "/home/javier/miniconda3/envs/mistral/lib/python310.zip\n",
      "/home/javier/miniconda3/envs/mistral/lib/python3.10\n",
      "/home/javier/miniconda3/envs/mistral/lib/python3.10/lib-dynload\n",
      "\n",
      "/home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import sys\n",
    "# he creado un modulo de herramientas con los enlaces simbólicos a los modulos que uso en los notebooks\n",
    "# insertar carpeta si no esta en sys.path\n",
    "if not '../herramientas/' in sys.path:\n",
    "    print(\"importando herramientas\")\n",
    "    sys.path.insert(0, '../herramientas/')\n",
    "import tunel_ssh_socks\n",
    "\n",
    "importlib.reload(tunel_ssh_socks)\n",
    "\n",
    "# ver todas las carpetas de sys.path y su orden de busqueda\n",
    "for path in sys.path:\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dirección IP: 83.138.42.107\\nPaís: ES\\nRegión: Valencia\\nCiudad: Alicante'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tunel_ssh_socks as tunel\n",
    "\n",
    "# listar todas las funciones del modulo\n",
    "# dir(tunel)\n",
    "tunel.obtener_informacion_ip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No funciona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ctransformers[cuda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03bee9ee573d4ff79ec376786f6c1ff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37c3cc58cba4f308c48a7e6a89e5bfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "\n",
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/deepseek-coder-6.7B-instruct-GGUF\", model_file=\"deepseek-coder-6.7b-instruct.Q6_K.gguf\", model_type=\"deepseek\", gpu_layers=50)\n",
    "\n",
    "print(llm(\"AI is going to\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's work this out in a step by step way to be sure we have the right answer.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/mnt/d/DeveloperIA/DeepSeekCoder/deepseek-coder-6.7b-instruct.Q6_K.gguf\",\n",
    "    temperature=0.75,\n",
    "    max_tokens=2000,\n",
    "    top_p=1,\n",
    "    # callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "write a quick sort algorithm in python.\n",
    "\"\"\"\n",
    "llm(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
