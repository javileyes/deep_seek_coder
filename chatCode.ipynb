{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usando Transformers con cuantificación de 8 bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para utilizar la cuantificación de 8 bits en modelos de Transformers, puedes usar el argumento `load_in_8bit=True` en el método `from_pretrained`. Esto te permite cargar un modelo reduciendo aproximadamente a la mitad los requisitos de memoria. Este método es compatible con modelos que admiten la carga con la biblioteca Accelerate y que contienen capas `torch.nn.Linear`.\n",
    "\n",
    "NOTA: Necesita tener instalado accelerete bitsandbytes para funcionar\n",
    "```bash\n",
    "pip install accelerete bitsandbytes\n",
    "```\n",
    "\n",
    "Para modificar tu script y aplicar cuantificación de 8 bits, puedes hacer lo siguiente:\n",
    "\n",
    "Este código utiliza cuantificación de 8 bits en lugar de la configuración original de 4 bits, lo que debería resultar en una reducción sustancial en el uso de memoria mientras mantiene un equilibrio adecuado entre eficiencia y precisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar modelo y ejecutar inferencia (mejor cargarlo una vez y luego ejecutarlo varias veces -en la siguiente sección)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5264e15957fe47bc84b99ecf29b1974d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here is a simple implementation of the Quick Sort algorithm in Python:\n",
      "\n",
      "```python\n",
      "def quick_sort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    else:\n",
      "        pivot = arr[0]\n",
      "        less_than_pivot = [x for x in arr[1:] if x <= pivot]\n",
      "        greater_than_pivot = [x for x in arr[1:] if x > pivot]\n",
      "        return quick_sort(less_than_pivot) + [pivot] + quick_sort(greater_than_pivot)\n",
      "\n",
      "# Test the function\n",
      "arr = [10, 7, 8, 9, 1, 5]\n",
      "print(\"Original array:\", arr)\n",
      "print(\"Sorted array:\", quick_sort(arr))\n",
      "```\n",
      "\n",
      "This code works by selecting a 'pivot' element from the array and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot. The pivot element is then in its final position. The process is then recursively applied to the sub-arrays.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", device_map='auto', load_in_8bit=True, trust_remote_code=True)\n",
    "\n",
    "#gracias al parámetro device_map='auto' en la función from_pretrained. Este parámetro le indica a la biblioteca Transformers que asigne automáticamente el modelo al mejor dispositivo disponible (normalmente la GPU, si hay una). Por lo tanto, no se necesita llamar explícitamente a .to('cuda')\n",
    "\n",
    "messages = [\n",
    "    {'role': 'user', 'content': \"write a quick sort algorithm in python.\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=32021)\n",
    "print(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aab7d25ccc14bc1904bd888d8cda82d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo cargado.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Inicializa la variable model como None al inicio\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "\n",
    "# Función para cargar el modelo si aún no está cargado\n",
    "def load_model():\n",
    "    global model\n",
    "    global tokenizer\n",
    "    if model is None or not hasattr(model, 'num_parameters'):  # Verifica si model está vacío o no parece ser un modelo válido\n",
    "        print(\"Cargando modelo...\")       \n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", device_map='auto', load_in_8bit=True, trust_remote_code=True)\n",
    "        print(\"Modelo cargado.\")\n",
    "    else:\n",
    "        print(\"Modelo ya estaba cargado.\")\n",
    "\n",
    "load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencia Modelo con gestión de chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo ya estaba cargado.\n",
      "El problema de las torres de Hanoi es un problema de recursividad. Consiste en mover un n�mero n de discos de un poste a otro, con la restricción de que un disco más grande no puede estar por encima de un disco más pequeño.\n",
      "\n",
      "Aquí está el algoritmo para resolver el problema de las torres de Hanoi:\n",
      "\n",
      "1. Mover n-1 discos de la torre origen a la torre auxiliar, usando la torre destino.\n",
      "2. Mover el disco restante de la torre origen a la torre destino.\n",
      "3. Mover los n-1 discos de la torre auxiliar a la torre destino usando la torre origen.\n",
      "\n",
      "Aquí está el pseudocódigo para el problema de las torres de Hanoi:\n",
      "\n",
      "```\n",
      "procedure hanoi(n, source, target, auxiliary)\n",
      "   if n > 0 then begin\n",
      "      // Mover los n-1 discos de la torre source a la torre auxiliary, usando target como torre auxiliar\n",
      "      hanoi(n - 1, source, auxiliary, target);\n",
      "      \n",
      "      // Mover el disco restante de la torre source a la torre target\n",
      "      move disk from source to target;\n",
      "      \n",
      "      // Mover los n-1 discos de la torre auxiliary a la torre target usando source como torre source\n",
      "      hanoi(n - 1, auxiliary, target, source);\n",
      "   end\n",
      "endprocedure\n",
      "```\n",
      "\n",
      "Ejemplo de ejecución:\n",
      "\n",
      "Si queremos mover 3 discos de la torre A a la torre C usando la torre B como torre auxiliar, el algoritmo se ejecutaría de la siguiente manera:\n",
      "\n",
      "1. Mover 2 discos de la torre A a la torre B usando la torre C como torre auxiliar.\n",
      "2. Mover el disco restante de la torre A a la torre C.\n",
      "3. Mover los 2 discos de la torre B a la torre C usando la torre A como torre source.\n",
      "4. Mover el disco restante de la torre A a la torre C.\n",
      "5. Mover 1 disco de la torre B a la torre C.\n",
      "\n",
      "Por lo tanto, para resolver el problema de las torres de Hanoi con 3 discos, se moverían 7 movimientos.\n",
      "\n",
      "Este es un ejemplo sencillo y para discos de tamaño diferente, el n�mero de movimientos seguirá siendo 2^n - 1, donde n es el n�mero de discos.\n",
      "\n",
      "\n",
      "################################################\n",
      "\n",
      "Sí, puedo hacerlo en Python. Aquí está la implementación del algoritmo de las torres de Hanoi en Python:\n",
      "\n",
      "```python\n",
      "def hanoi(n, source, target, auxiliary):\n",
      "    if n > 0:\n",
      "        # Mover los n-1 discos de la torre source a la torre auxiliary, usando target como torre auxiliar\n",
      "        hanoi(n - 1, source, auxiliary, target)\n",
      "        \n",
      "        # Mover el disco restante de la torre source a la torre target\n",
      "        print(f\"Mover disco {n} de la torre {source} a la torre {target}\")\n",
      "        \n",
      "        # Mover los n-1 discos de la torre auxiliary a la torre target usando source como torre source\n",
      "        hanoi(n - 1, auxiliary, target, source)\n",
      "\n",
      "# Ejecutar el algoritmo con 3 discos\n",
      "hanoi(3, 'A', 'C', 'B')\n",
      "```\n",
      "\n",
      "En este código, `hanoi` es una función recursiva que resuelve el problema de las torres de Hanoi. La función recibe cuatro parámetros: el n�mero de discos `n`, la torre origen `source`, la torre destino `target` y la torre auxiliar `auxiliary`.\n",
      "\n",
      "La función imprime los pasos para mover cada disco, de la forma \"Mover disco `n` de la torre `source` a la torre `target`\".\n",
      "\n",
      "Por ejemplo, si ejecutamos `hanoi(3, 'A', 'C', 'B')`, el programa imprimirá los siguientes pasos:\n",
      "\n",
      "```\n",
      "Mover disco 1 de la torre A a la torre B\n",
      "Mover disco 2 de la torre A a la torre C\n",
      "Mover disco 1 de la torre B a la torre C\n",
      "Mover disco 3 de la torre A a la torre C\n",
      "Mover disco 1 de la torre C a la torre B\n",
      "Mover disco 2 de la torre C a la torre A\n",
      "Mover disco 1 de la torre B a la torre A\n",
      "```\n",
      "\n",
      "Estos son los pasos para mover 3 discos de la torre A a la torre C usando la torre B como torre auxiliar.\n",
      "\n",
      "\n",
      "################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "\n",
    "def ajustar_contexto(texto, max_longitud=2000, secuencia=\"### Instruction\"):\n",
    "    # Comprobar si la longitud del texto es mayor que el máximo permitido\n",
    "    if len(texto) > max_longitud:\n",
    "        # Buscar la secuencia de ajuste\n",
    "        indice_secuencia = texto.find(secuencia)\n",
    "\n",
    "        # Si la secuencia existe\n",
    "        if indice_secuencia != -1:\n",
    "            # Retornar el texto desde la secuencia en adelante\n",
    "            return texto[indice_secuencia:]\n",
    "        else:\n",
    "            # Si la secuencia no se encuentra, recortar simplemente a la longitud máxima\n",
    "            return texto[:max_longitud]\n",
    "    else:\n",
    "        return texto\n",
    "\n",
    "# Ejemplo de uso de la función\n",
    "# texto_engordado = \"Este es el texto adicional que podría hacer que el texto sobrepase los 30 caracteres. ### Instruction Continuación del texto...\"\n",
    "# texto_ajustado = ajustar_contexto(texto_engordado, 30)\n",
    "# print(texto_ajustado)\n",
    "\n",
    "def generate_long_chat(historico, input_text, max_additional_tokens=2000, stop=[\"<|EOT|>\"]):\n",
    "\n",
    "\n",
    "    prompt = f\"### Instruction:\\n{input_text}\\n### Response:\\n\"\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True) # para streamear el output pero sin repetir el prompt ni el contexto anterior. \n",
    "\n",
    "    final_prompt = historico + \"\\n\" + prompt\n",
    "    longitud_prompt_tokens = len(tokenizer.encode(final_prompt))\n",
    "\n",
    "    inputs = tokenizer(final_prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "\n",
    "    model_inputs = inputs.to(model.device)      # .to(\"cuda\")\n",
    "    outputs = model.generate(**model_inputs,\n",
    "                             streamer=streamer,\n",
    "                             max_new_tokens=max_additional_tokens,\n",
    "                            #  max_length=max_length,\n",
    "                             temperature=0.1,\n",
    "                             pad_token_id = 3200,\n",
    "                             eos_token_id=32021,\n",
    "                             do_sample=True)\n",
    "\n",
    "    # Decodificar el tensor de salida a una cadena de texto\n",
    "    inicio_generado = longitud_prompt_tokens - 1\n",
    "    decoded_output = tokenizer.decode(outputs[0][inicio_generado:], skip_special_tokens=True)  \n",
    "    # decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)    \n",
    "    \n",
    "    text = final_prompt + decoded_output + \"<|EOT|>\"\n",
    "    return text\n",
    "\n",
    "\n",
    "# para que pueda tener suficiente memoria si queremos algo más de contexto.\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", device_map='auto', load_in_8bit=True, trust_remote_code=True)\n",
    "\n",
    "# modelo sin cuantizar (se queda sin memoria con contexto grande)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", torch_dtype=\"auto\", trust_remote_code=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", device_map='auto', torch_dtype=\"auto\", trust_remote_code=True)\n",
    "#torch_dtype=torch.float16  (half precision) or torch.float32 (single precision que sería absurdo porque deepseek coder viene de llama 2 cuyo parámetros son float16)\n",
    "\n",
    "load_model()\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are an expert AI programming assistant, utilizing the DeepSeek Coder model, and you only answer questions related to computer science.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "historico = system_prompt\n",
    "\n",
    "while True:\n",
    "    # read input\n",
    "    input_text = input(\"user: \")\n",
    "    if input_text == \"/exit\": break\n",
    "    if input_text == \"/historico\": \n",
    "        print(historico)\n",
    "        continue\n",
    "    if input_text == \"/len\": \n",
    "        print(\"longitud del contexto en caracteres: \", len(historico))\n",
    "        continue\n",
    "    if input_text == \"/clear\":\n",
    "        historico = \"\"\n",
    "        continue\n",
    "    # generate response\n",
    "    historico = generate_long_chat(historico, input_text=input_text, max_additional_tokens=2048)\n",
    "    historico = ajustar_contexto(historico)\n",
    "    # print response\n",
    "    # print(salida)\n",
    "    print(f\"\\n################################################\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usando Transformers con cuantificación de 4 bits (BitsandBytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La librería Bits and Bytes (BitsAndBytes) es una herramienta que facilita la optimización de modelos de lenguaje grandes (como GPT-3 y similares) para su ejecución en hardware con recursos limitados. Estas optimizaciones incluyen técnicas como la cuantificación a 4 bits y el uso de tipos de datos de menor precisión para reducir el uso de memoria y mejorar la eficiencia de la inferencia. \n",
    "\n",
    "Veamos en detalle los parámetros que mencionaste en tu configuración de `BitsAndBytesConfig`:\n",
    "\n",
    "1. **`load_in_4bit`**: \n",
    "   - Este parámetro, cuando se establece en `True`, habilita la cuantificación a 4 bits. La cuantificación es una técnica para reducir la precisión de los números usados en un modelo (por ejemplo, de 32 bits a 4 bits). Esto disminuye el tamaño del modelo y puede reducir el uso de memoria, pero potencialmente a costa de alguna pérdida de precisión en los cálculos.\n",
    "\n",
    "2. **`bnb_4bit_quant_type`**:\n",
    "   - Define el tipo de cuantificación a 4 bits a utilizar. El valor `'nf4'` se refiere a una forma específica de cuantificación a 4 bits desarrollada por el equipo de BitsAndBytes. Esta forma de cuantificación está diseñada para mantener un equilibrio entre el rendimiento y la precisión.\n",
    "\n",
    "3. **`bnb_4bit_use_double_quant`**:\n",
    "   - Cuando se establece en `True`, activa el uso de \"doble cuantificación\" en el proceso de cuantificación a 4 bits. Esto significa que se aplican dos rondas de cuantificación en lugar de una, lo que puede mejorar la precisión de la representación de 4 bits a costa de una ligera sobrecarga computacional.\n",
    "\n",
    "4. **`bnb_4bit_compute_dtype`**:\n",
    "   - Este parámetro especifica el tipo de dato a utilizar para los cálculos internos del modelo cuando se utiliza la cuantificación a 4 bits. El tipo de dato `bfloat16` es una representación de punto flotante de 16 bits con menos precisión que el estándar de 32 bits (`float32`), pero con un rango comparable. Usar `bfloat16` puede ofrecer un buen equilibrio entre el rendimiento y la precisión, y es especialmente útil en hardware que soporta operaciones de `bfloat16` de manera eficiente.\n",
    "\n",
    "En resumen, estos parámetros están configurando tu modelo para que use cuantificación a 4 bits con un método específico y una doble cuantificación para mejorar la precisión, todo mientras realiza cálculos internos en un tipo de dato de menor precisión (`bfloat16`) para mejorar la eficiencia. Estas optimizaciones están diseñadas para permitir que modelos grandes se ejecuten en hardware con memoria limitada, como GPUs con menos VRAM, manteniendo un buen equilibrio entre el rendimiento y la precisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d656cf3bf984a6bbd2f040c9f4d966a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here is a simple implementation of the Quick Sort algorithm in Python:\n",
      "\n",
      "```python\n",
      "def quick_sort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    else:\n",
      "        pivot = arr[len(arr) // 2]\n",
      "        left = [x for x in arr if x < pivot]\n",
      "        middle = [x for x in arr if x == pivot]\n",
      "        right = [x for x in arr if x > pivot]\n",
      "        return quick_sort(left) + middle + quick_sort(right)\n",
      "\n",
      "# Test the function\n",
      "print(quick_sort([3,6,8,10,1,2,1]))\n",
      "```\n",
      "\n",
      "This function works by selecting a pivot element from the array and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot. The sub-arrays are then recursively sorted.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from torch import bfloat16\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", quantization_config=bnb_config,     device_map='auto', trust_remote_code=True)\n",
    "messages=[\n",
    "    { 'role': 'user', 'content': \"write a quick sort algorithm in python.\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
    "# 32021 is the id of <|EOT|> token\n",
    "outputs = model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=32021)\n",
    "print(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usando Transformers con cuantificación de 16 bits (convirtiendo a half-precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al usar parámetro torch_dtype=\"auto\"  en la función from_pretrained() de la biblioteca Transformers de Hugging Face es una funcionalidad relativamente reciente que permite a la función determinar automáticamente el tipo de datos de PyTorch (dtype) más adecuado para cargar el modelo, basándose en el hardware disponible.\n",
    "\n",
    "Cuando configuras torch_dtype en \"auto\", la biblioteca intentará elegir el mejor tipo de datos para optimizar el rendimiento y la eficiencia de memoria. Por ejemplo, si se detecta una GPU compatible con half-precision (como las GPUs con soporte para Tensor Cores de NVIDIA), puede cargar automáticamente el modelo en \"float16\" para aprovechar la aceleración de half-precision. Si no se detecta dicho hardware, utilizará el tipo de datos estándar (usualmente float32).\n",
    "\n",
    "Automáticamente se carga usando half-precision (16 bits) cuando se usa la biblioteca Accelerate. Esto se debe a que Accelerate utiliza la biblioteca NVIDIA Apex para la conversión automática a half-precision. Puedes ver esto en la documentación de Accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: accelerate\n",
      "Version: 0.24.1\n",
      "Summary: Accelerate\n",
      "Home-page: https://github.com/huggingface/accelerate\n",
      "Author: The HuggingFace team\n",
      "Author-email: sylvain@huggingface.co\n",
      "License: Apache\n",
      "Location: /home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages\n",
      "Requires: huggingface-hub, numpy, packaging, psutil, pyyaml, torch\n",
      "Required-by: peft\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la biblioteca Transformers de Hugging Face, los valores para `device_map` y `torch_dtype` se pueden definir de la siguiente manera:\n",
    "\n",
    "1. **`device_map`**:\n",
    "   - **`'auto'`**: El modelo se distribuye automáticamente a través de los dispositivos disponibles de manera óptima. Esto es útil si tienes múltiples GPUs y deseas que la biblioteca maneje la distribución del modelo de manera eficiente.\n",
    "   - **Diccionario específico de dispositivos**: Puedes proporcionar un diccionario que asigne capas específicas del modelo a dispositivos específicos. Por ejemplo, `{0: [0, 1], 1: [2, 3]}` asignaría las capas 0 y 1 a la GPU 0 y las capas 2 y 3 a la GPU 1.\n",
    "   - **Lista de IDs de dispositivos**: También puedes proporcionar una lista de IDs de dispositivos para un reparto más manual.\n",
    "\n",
    "2. **`torch_dtype`**:\n",
    "   - **`'auto'`**: Selecciona automáticamente el tipo de datos de PyTorch más adecuado en función del hardware disponible y el modelo específico.\n",
    "   - **Tipos de datos específicos de PyTorch**: Puedes especificar un tipo de datos concreto como `torch.float32`, `torch.float16`, etc., para controlar la precisión y el uso de memoria del modelo.\n",
    "\n",
    "Estos parámetros proporcionan flexibilidad en la configuración del hardware y el rendimiento del modelo, permitiendo optimizaciones según el entorno de ejecución y los requisitos específicos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí en un modelo de 32 capas típico Llama asignamos la mitad de las capas a la GPU y la otra mitad a la CPU:\n",
    "```python\n",
    "device_map = {\n",
    "    \"cuda:0\": [0, 1, 2, ..., 15],  # Primeras 16 capas en la GPU\n",
    "    \"cpu\": [16, 17, 18, ..., 31]   # Últimas 16 capas en la CPU\n",
    "}\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\"tu-modelo\", device_map=device_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## usamos device_map='auto' para que la biblioteca maneje la distribución del modelo de manera eficiente.\n",
    "Esta manera es muy sencilla, intenta poner el modelo en el mejor dispositivo PERO si no cupiera todo el modelo en la GPU daría un error. Con este parámetro a \"auto\" se asigna un disposivivo que se puede verse con \"model.device\" \n",
    "\n",
    "Otra forma de gestionar sería sin usar el parámetro device_map, entonces todo se cargaría en la RAM y a partir de ahí se podría usar instrucciones como:\n",
    "```python\n",
    "model = model.to(dtype=torch.float16, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "```\n",
    "para asignar el tipo de dato y el dispositivo donde volcar el modelo.\n",
    "Esta forma postergaría tanto la cuantificación a half_precision como la carga a la GPU hasta esta linea.\n",
    "\n",
    "Si no tenemos memoria suficiente en la RAM se podría usar \"dtype=torch.float16\" O \"dtype='auto'\" durante la carga del modelo y luego usar\n",
    "```python\n",
    "model = model.to(device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "```\n",
    "para trasladar el modelo a la GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mínima cuantificación para mayor precisión: \"torch_dtype=torch.float16\" o \"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee8ce34f4c3c4b8e9969e2868cbd562c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", torch_dtype=\"auto\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", device_map='auto', torch_dtype=\"auto\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para ver el formato que usa: https://github.com/deepseek-ai/deepseek-coder\n",
    "\n",
    "```bash\n",
    "You are an AI programming assistant, utilizing the DeepSeek Coder model, developed by DeepSeek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer.\n",
    "### Instruction:\n",
    "['content']\n",
    "### Response:\n",
    "['content']\n",
    "<|EOT|>\n",
    "### Instruction:\n",
    "['content']\n",
    "### Response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer.\n",
      "### Instruction:\n",
      "write a quick sort algorithm in python.\n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'user', 'content': \"write a quick sort algorithm in python.\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Decodificar el tensor completo a texto\n",
    "text = tokenizer.decode(inputs.flatten(), skip_special_tokens=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ver capas del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo tiene 32 capas.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "def count_model_layers(model):\n",
    "    #En nuestro caso entrará en este primer if\n",
    "    if hasattr(model, 'model') and hasattr(model.model, 'layers'):\n",
    "        return len(model.model.layers)\n",
    "    elif hasattr(model, 'encoder'):\n",
    "        return len(model.encoder.layer)\n",
    "    elif hasattr(model, 'transformer'):\n",
    "        return len(model.transformer.h)\n",
    "    elif hasattr(model, 'decoder'):\n",
    "        return len(model.decoder.block)\n",
    "    else:\n",
    "        return \"No se puede determinar el número de capas\"\n",
    "\n",
    "\n",
    "print(f\"El modelo tiene {count_model_layers(model)} capas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usamos device_map \"a medida\" para intentar que el modelo \"quepa\" en la GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No he logrado que funcionen para este modelo, dan error, solo funcionan con \"auto\" (investigar más)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97cc50a2dff3444d980f6464ba9da318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device_map = {\n",
    "    \"cuda:0\": [0, 1, 2, ..., 15],  # Primeras 16 capas en la GPU\n",
    "    \"cpu\": [16, 17, 18, ..., 31]   # Últimas 16 capas en la CPU\n",
    "}\n",
    "device_map = {\n",
    "    \"cuda:0\": [0, 1, 2, ..., 15],  # Primeras 16 capas en la GPU\n",
    "    \"cpu\": \"default\"   # resto de capas en la CPU\n",
    "}\n",
    "\n",
    "device_map = {\"cuda:0\": \"default\"}\n",
    "\n",
    "#SOLO CONSIGO QUE FUNCIONE \"DEVICE_MAP\" CON AUTO (TENGO QUE INVESTIGAR MÁS)\n",
    "\n",
    "device_map = \"auto\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", torch_dtype=\"auto\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", device_map=device_map, torch_dtype=\"auto\", trust_remote_code=True)\n",
    "\n",
    "\n",
    "# model = LlamaForCausalLM.from_pretrained(\"tu-modelo\", device_map=device_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otra forma de hacerlo es convertirlo a half-precision después de cargarlo pero necesitas tener más memoria RAM disponible.\n",
    "Teniendo la mitad de VRAM (GPU) que de RAM se podría cargar un modelo grande y después convertirlo a half-precision (16 bits) para poder ejecutarlo en la GPU.\n",
    "\n",
    "Para modificar tu script para usar cuantificación de 16 bits (también conocida como precisión mixta o half-precision), puedes convertir el modelo a float16 después de cargarlo. Esto se puede hacer utilizando el método .to() disponible en modelos de PyTorch. Aquí está el código ajustado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True)\n",
    "\n",
    "# Convertir el modelo a float16 y transferirlo a la GPU si está disponible\n",
    "model = model.to(dtype=torch.float16, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "messages = [\n",
    "    {'role': 'user', 'content': \"write a quick sort algorithm in python.\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=32021)\n",
    "print(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencia de DeepSeek Coder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a Python implementation of the quick sort algorithm:\n",
      "\n",
      "```python\n",
      "def quickSort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    pivot = arr[len(arr) // 2]\n",
      "    left = [x for x in arr if x < pivot]\n",
      "    middle = [x for x in arr if x == pivot]\n",
      "    right = [x for x in arr if x > pivot]\n",
      "    return quickSort(left) + middle + quickSort(right)\n",
      "\n",
      "print(quickSort([3,6,8,10,1,2,1]))\n",
      "# Output: [1, 1, 2, 3, 6, 8, 10]\n",
      "```\n",
      "\n",
      "The quick sort algorithm works by selecting a 'pivot' element from the array and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot. The sub-arrays are then recursively sorted.\n",
      "\n",
      "This implementation is a bit more complex than necessary for a quick sort, because it creates additional lists. A more space-efficient version of quick sort might involve swapping elements in-place, although this can be more complex to implement.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'user', 'content': \"write a quick sort algorithm in python.\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(inputs, max_new_tokens=512, do_sample=True, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=32021)\n",
    "print(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## uso de streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a basic implementation of Quick Sort algorithm in Python:\n",
      "\n",
      "```python\n",
      "def quicksort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    else:\n",
      "        pivot = arr[0]\n",
      "        less_than_pivot = [x for x in arr[1:] if x <= pivot]\n",
      "        greater_than_pivot = [x for x in arr[1:] if x > pivot]\n",
      "        return quicksort(less_than_pivot) + [pivot] + quicksort(greater_than_pivot)\n",
      "\n",
      "# Test the function\n",
      "arr = [3, 6, 8, 10, 1, 2, 1]\n",
      "print(\"Original Array: \", arr)\n",
      "print(\"Sorted Array: \", quicksort(arr))\n",
      "```\n",
      "\n",
      "In this implementation, the `quicksort` function takes in a list `arr` and:\n",
      "\n",
      "- If the list is empty or contains only one element, it is already sorted, so it just returns the list.\n",
      "- It selects the first element of the list as the pivot and creates two lists: `less_than_pivot` and `greater_than_pivot`.\n",
      "- It then recursively sorts the `less_than_pivot` and `greater_than_pivot` lists, and combines the three lists to form the final sorted array.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True) # para streamear el output pero sin repetir el prompt ni el contexto anterior.\n",
    "\n",
    "content = \"\"\"\n",
    "write a quick sort algorithm in python.\n",
    "\"\"\"\n",
    "messages = [\n",
    "    {'role': 'user', 'content': content}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(inputs, max_new_tokens=512, streamer=streamer, do_sample=True, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=32021)\n",
    "# print(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varias secuencias de salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secuencia 1:\n",
      "Here is a quick sort algorithm implemented in Python:\n",
      "\n",
      "```python\n",
      "def quickSort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    else:\n",
      "        pivot = arr[len(arr) // 2]\n",
      "        left = [x for x in arr if x < pivot]\n",
      "        middle = [x for x in arr if x == pivot]\n",
      "        right = [x for x in arr if x > pivot]\n",
      "        return quickSort(left) + middle + quickSort(right)\n",
      "\n",
      "# Testing the function\n",
      "print(quickSort([3,6,8,10,1,2,1]))\n",
      "# Output: [1, 1, 2, 3, 6, 8, 10]\n",
      "```\n",
      "\n",
      "The quickSort function works by selecting a 'pivot' element from the array and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot. The sub-arrays are then recursively sorted.\n",
      "\n",
      "\n",
      "Secuencia 2:\n",
      "Sure, here is a Python implementation of the Quick Sort algorithm:\n",
      "\n",
      "```python\n",
      "def quicksort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    pivot = arr[len(arr) // 2]\n",
      "    left = [x for x in arr if x < pivot]\n",
      "    middle = [x for x in arr if x == pivot]\n",
      "    right = [x for x in arr if x > pivot]\n",
      "    return quicksort(left) + middle + quicksort(right)\n",
      "\n",
      "print(quicksort([3,6,8,10,1,2,1]))\n",
      "```\n",
      "\n",
      "This algorithm works by selecting a 'pivot' element from the array and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot. The sub-arrays are then recursively sorted.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "content = \"\"\"\n",
    "write a quick sort algorithm in python.\n",
    "\"\"\"\n",
    "messages = [\n",
    "    {'role': 'user', 'content': content}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(inputs, max_new_tokens=512, do_sample=True, top_k=50, top_p=0.95, num_return_sequences=2, eos_token_id=32021)\n",
    "\n",
    "print(\"Secuencia 1:\")\n",
    "print(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))\n",
    "\n",
    "print(\"\\nSecuencia 2:\")\n",
    "print(tokenizer.decode(outputs[1][len(inputs[0]):], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Busqueda de la mejor secuencia de salida (Beam Search) [tarda bastante más pero puede ser más precisa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secuencia 1:\n",
      "Sure, here is a simple implementation of the Quick Sort algorithm in Python:\n",
      "\n",
      "```python\n",
      "def quick_sort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    else:\n",
      "        pivot = arr[0]\n",
      "        less_than_pivot = [x for x in arr[1:] if x <= pivot]\n",
      "        greater_than_pivot = [x for x in arr[1:] if x > pivot]\n",
      "        return quick_sort(less_than_pivot) + [pivot] + quick_sort(greater_than_pivot)\n",
      "\n",
      "# Test the function\n",
      "print(quick_sort([3, 6, 8, 10, 1, 2, 1]))  # Output: [1, 1, 2, 3, 6, 8, 10]\n",
      "```\n",
      "\n",
      "This algorithm works by selecting a 'pivot' element from the array and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot. The sub-arrays are then recursively sorted.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "content = \"\"\"\n",
    "write a quick sort algorithm in python.\n",
    "\"\"\"\n",
    "messages = [\n",
    "    {'role': 'user', 'content': content}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(inputs, max_new_tokens=512, do_sample=True, top_k=50, top_p=0.95, num_beams=4, num_return_sequences=1, eos_token_id=32021)\n",
    "\n",
    "print(\"Secuencia 1:\")\n",
    "print(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usando llama-cpp-python (No consigo usar la GPU, uso Docker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "docker run  -it -p 8000:8000 --name deepseek -v /mnt/d/DeveloperIA/DeepSeekCoder/models:/models -e MODEL=/models/deepseek-coder-6.7b-instruct.Q6_K.gguf ghcr.io/abetlen/llama-cpp-python:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "def partition(array, low, high):\n",
      "    i = (low-1) # index of smaller element\n",
      "    pivot = array[high] # pivot\n",
      "  \n",
      "    for j in range(low , high): \n",
      "        if array[j] <= pivot: \n",
      "            i += 1\n",
      "            array[i],array[j] = array[j],array[i]\n",
      "  \n",
      "    array[i+1], array[high] = array[high], array[i+1] \n",
      "    return (i+1) \n",
      "  \n",
      "def quickSort(array, low, high): \n",
      "    if low < high: \n",
      "        pi = partition(array,low,high) \n",
      "        quickSort(array, low, pi-1) \n",
      "        quickSort(array, pi+1, high)\n",
      "\n",
      "data = [8, 7, 2, 1, 0, 9, 6]\n",
      "size = len(data) \n",
      "quickSort(data, 0, size - 1) \n",
      "print('Sorted array is:') \n",
      "print(data)\n",
      "<jupyter_output>\n",
      "Sorted array is:\n",
      "[0, 1, 2, 6, 7, 8, 9]\n",
      "<jupyter_text>\n",
      "Task-3: To Explore Unsupervised Machine Learning : K Means Clustering   Importing the libraries and dataset\n",
      "<jupyter_code>\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "from sklearn import datasets\n",
      "df = pd.read_csv('Iris.csv')\n",
      "print(\"Data imported successfully\")\n",
      "df.head()\n",
      "<jupyter_output>\n",
      "Data imported successfully\n",
      "<jupyter_text>\n",
      "Preprocessing the data\n",
      "<jupyter_code>\n",
      "x = df.iloc[:, [0, 1, 2, 3]].values #Getting values from columns\n",
      "#No need to encode categorical variables in this dataset as it's iris dataset which has no such variable\n",
      "<jupyter_output>\n",
      "<empty_output>\n",
      "<jupyter_text>\n",
      "Finding the optimal number of clusters using Elbow\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "prompt = \"write a quick sort algorithm in python.\" \n",
    "\n",
    "response = requests.post(\"http://localhost:8000/v1/completions\", json={\n",
    "    \"prompt\": prompt,\n",
    "    \"max_tokens\": 512,\n",
    "    \"eos_token_id\": 32021\n",
    "})\n",
    "\n",
    "print(response.json()[\"choices\"][0][\"text\"])\n",
    "# print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprobar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(arr, low, high):\n",
    "    i = (low-1)\n",
    "    pivot = arr[high]\n",
    "\n",
    "    for j in range(low, high):\n",
    "        if arr[j] <= pivot:\n",
    "            i = i+1\n",
    "            arr[i], arr[j] = arr[j], arr[i]\n",
    "\n",
    "    arr[i+1], arr[high] = arr[high], arr[i+1]\n",
    "    return (i+1)\n",
    "\n",
    "def quickSort(arr, low, high):\n",
    "    if len(arr) == 1:\n",
    "        return arr\n",
    "    if low < high:\n",
    "        pi = partition(arr, low, high)\n",
    "        quickSort(arr, low, pi-1)\n",
    "        quickSort(arr, pi+1, high)\n",
    "\n",
    "arr = [10, 7, 8, 9, 1, 5]\n",
    "n = len(arr)\n",
    "quickSort(arr, 0, n-1)\n",
    "print(\"Sorted array is:\", arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_sort(arr):\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "    else:\n",
    "        pivot = arr[len(arr) // 2]\n",
    "        left = [x for x in arr if x < pivot]\n",
    "        middle = [x for x in arr if x == pivot]\n",
    "        right = [x for x in arr if x > pivot]\n",
    "        return quick_sort(left) + middle + quick_sort(right)\n",
    "\n",
    "# Example usage:\n",
    "arr = [3, 6, 8, 10, 1, 2, 1]\n",
    "print(quick_sort(arr))  # prints \"[1, 1, 2, 3, 6, 8, 10]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tunel y proxy Socks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importando herramientas\n",
      "../herramientas/\n",
      "/mnt/d/DeveloperIA/DeepSeekCoder\n",
      "/home/javier/miniconda3/envs/mistral/lib/python310.zip\n",
      "/home/javier/miniconda3/envs/mistral/lib/python3.10\n",
      "/home/javier/miniconda3/envs/mistral/lib/python3.10/lib-dynload\n",
      "\n",
      "/home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import sys\n",
    "# he creado un modulo de herramientas con los enlaces simbólicos a los modulos que uso en los notebooks\n",
    "# insertar carpeta si no esta en sys.path\n",
    "if not '../herramientas/' in sys.path:\n",
    "    print(\"importando herramientas\")\n",
    "    sys.path.insert(0, '../herramientas/')\n",
    "import tunel_ssh_socks\n",
    "\n",
    "importlib.reload(tunel_ssh_socks)\n",
    "\n",
    "# ver todas las carpetas de sys.path y su orden de busqueda\n",
    "for path in sys.path:\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dirección IP: 83.138.42.107\\nPaís: ES\\nRegión: Valencia\\nCiudad: Alicante'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tunel_ssh_socks as tunel\n",
    "\n",
    "# listar todas las funciones del modulo\n",
    "# dir(tunel)\n",
    "tunel.obtener_informacion_ip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No funciona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ctransformers[cuda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03bee9ee573d4ff79ec376786f6c1ff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37c3cc58cba4f308c48a7e6a89e5bfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "\n",
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/deepseek-coder-6.7B-instruct-GGUF\", model_file=\"deepseek-coder-6.7b-instruct.Q6_K.gguf\", model_type=\"deepseek\", gpu_layers=50)\n",
    "\n",
    "print(llm(\"AI is going to\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's work this out in a step by step way to be sure we have the right answer.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/mnt/d/DeveloperIA/DeepSeekCoder/deepseek-coder-6.7b-instruct.Q6_K.gguf\",\n",
    "    temperature=0.75,\n",
    "    max_tokens=2000,\n",
    "    top_p=1,\n",
    "    # callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "write a quick sort algorithm in python.\n",
    "\"\"\"\n",
    "llm(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
